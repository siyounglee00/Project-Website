<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>Is YouTube a Mirror of Society?</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="./img/epfl.png" rel="icon">
  <link href="./img/epfl.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">
  <link href="assets/css/ourMark.css" rel="stylesheet">

  <!-- AOS CSS -->
  <link href="https://unpkg.com/aos@2.3.1/dist/aos.css" rel="stylesheet">

  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css" />

  <!-- Tailwind Library -->
  <link rel="stylesheet"
    href="https://cdn.jsdelivr.net/gh/creativetimofficial/tailwind-starter-kit/compiled-tailwind.min.css" />

  <!-- =======================================================
  * Template Name: MyResume - v4.7.0
  * Template URL: https://bootstrapmade.com/free-html-bootstrap-template-my-resume/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

  <!-- ======= Mobile nav toggle button ======= -->
  <!-- <button type="button" class="mobile-nav-toggle d-xl-none"><i class="bi bi-list mobile-nav-toggle"></i></button> -->
  <i class="bi bi-list mobile-nav-toggle d-xl-none"></i>
  <!-- ======= Header ======= -->
  <header id="header" class="d-flex flex-column justify-content-center">

    <nav id="navbar" class="navbar nav-menu">
      <ul>
        <li><a href="#hero" class="nav-link scrollto active"><i class="bx bx-home"></i> <span>Home</span></a></li>
        <li><a href="#intro" class="nav-link scrollto"><i class="bx bxs-megaphone"></i> <span>Introduction</span></a></li>
        <li><a href="#data" class="nav-link scrollto"><i class="bx bx-data"></i> <span>Dataset Description</span></a></li>
        <li><a href="#ngrams" class="nav-link scrollto"><i class="fa-regular fa-file-lines"></i> <span>N-grams Analysis</span></a></li>
        <li><a href="#lda" class="nav-link scrollto"><i class="fa-sharp fa-solid fa-code-compare"></i> <span>Latent Dirichlet Allocation</span></a></li>
        <li><a href="#timeseries" class="nav-link scrollto"><i class="bx bx-line-chart"></i> <span>Time Series Analysis</span></a></li>
        <li><a href="#discussion" class="nav-link scrollto"><i class="fas fa-comments"></i><span>Discussion</span></a></li>
        <li><a href="#conclusion" class="nav-link scrollto"><i class="fa-solid fa-question"></i> <span>Conclusion</span></a></li>
        <li><a href="#links" class="nav-link scrollto"><i class="bx bx-link"></i> <span>Authors & Links</span></a></li>
      </ul>
    </nav><!-- .nav-menu -->

  </header><!-- End Header -->

  <!-- ======= Hero Section ======= -->
  <section id="hero" class="d-flex flex-column justify-content-center">
    <div class="container" data-aos="zoom-in" data-aos-delay="100">
      <p>Is YouTube a mirror of <span class="typed" data-typed-items="Society?, the News?, the World?"></span></p>
      <p>
          <small class="ml-3"><b>By:</b></small>
          <small class="ml-3">Simon Lee,</small>
          <small class="ml-3">Arben Miftari,</small>
          <small class="ml-3">Siyoung Lee,</small>
          <small class="ml-3">Lina Bacha</small>
        </p>
        <p style="font-size: 16px; color:#FFCE44">Team: <b>chromeGoldFish</B></p>

    </div>
  </section><!-- End Hero -->

  <main id="main">

    <!-- ======= Introduction Section ======= -->
    <section id="intro" class="about smallerWidthDisplay">
      <div class="container" data-aos="fade-up">

        <div class="section-title">
          <h2>Introduction</h2>
          <p style="text-align: left;">
            YouTube has grown into a major source of information over the decade, with its main attributes being selectivity and anonymity. 
            According to a study by <a href="https://www.pewresearch.org/journalism/2020/09/28/many-americans-get-news-on-youtube-where-news-organizations-and-independent-producers-thrive-side-by-side/" target="_blank">G.Stocking et al.(2020)</a>,
            a quarter of US citizens consider YouTube as being an important news outlet. 
            And with technology becoming more accessible year after year, the user rate for YouTube has naturally grown. 
            Roughly 30% of our global population has used YouTube for a wide variety of unknown reasons, and these numbers are expected to grow every year. 
            Below we present some user information from the <a href="https://www.globalmediainsight.com/blog/youtube-users-statistics/#year" target="_blank">Global Media Insight</a> group where we obtain our following statistics and future forecasts of global YouTube usage.
            <br></br>

            <div class="youtube usage">
              <div id="map1" class="map_containers">
                <!-- Title above map -->
                <object type="text/html" data="./img/user.html" align="left" width=900, height=500></object>
              </div>
            </div>
            
            <p style="text-align: left;">
              What we see above is not very shocking due to the increased digitization of our modern world. 
              However, this massive usage rate across the globe leads us to ask what exactly is YouTube’s intended purpose. According to YouTube itself, quote,
              <div style="text-align: center; width: 80%; margin: auto; margin-top: 1em; margin-bottom: 3em;">
                <i class="bx bxs-quote-alt-left quote-icon-left"></i>
                <em>Our mission is to give everyone a voice and show them the world. We believe that everyone deserves to have a voice, and that the world is a better place when we listen, share and build community through our stories.</em>
                <i class="bx bxs-quote-alt-right quote-icon-right"></i>
              </div>
            </p>

            <p style="text-align: left;">
              Like most online platforms, YouTube has great intentions for its service, yet it still provokes numerous consequences. 
              One major concern for a video-sharing platform with no uploading rules is the regulation of content and who it's able to reach through its service. 
              It is safe to say that our generation has become “numb” to the internet, and these concerns often get overlooked. 
              We have become so numb to the point that we underestimate the internet's true capabilities of spreading any type of possibly non-factual malicious information anywhere. 
              But why is this a concern if there is no harm associated with it?

              <br></br>

              To answer this, we redirect your attention to the second graph, which extracts YouTube’s usage by country. 
              This is very informative because we can see how dominant the platform has become in various countries relative to their population. 
              It has come to the point that even YouTube Creators have noticed this incredible reach and have exploited it for the better and the worse. 
              For example, Donald Trump’s advertisement team pumped out numerous YouTube advertisements and videos during his 2016 presidential campaign, knowing that nearly 75% of Americans utilized YouTube routinely. 
              So with information sourced from <a href="https://www.demandsage.com/youtube-stats/" target="_blank">Demands Age</a>, we plot the top 15 countries that have the highest YouTube usage along with their relative population.

              <br></br>
            </p>

            <div class="youtube usage">
              <div id="map1" class="map_containers">
                <!-- Title above map -->
                <object type="text/html" data="./img/country.html" align="left" width=900, height=500></object>
              </div>
            </div>

            <p style="text-align: left;">
              While these two introductory histograms are telling of YouTube's surge throughout the years, it remains difficult to map what type of information users access from this web service. 
              However, instead of pondering on what we don’t know, our project focuses on what we do know: 
              which are the “momentous” events/trends that occur within our daily lives that help develop our growing human history (e.g. Fortnite, Ebola Epidemic, TikTok, etc.). 
              Therefore, our project aim is trying to answer whether <strong>YouTube is a mirror of our society?</strong> 
              To obtain a proper answer, we will prioritize the two following methodologies to conduct our analysis from our YouTube dataset:

            </p>
            <br></br>

            <div style="display: flex; margin: auto; width: 80%; margin-bottom: 2em; margin-top: 2em;">
              <div style="width: 60%;">
                <img src="./img/2012_unigram.png" class="img-fluid" alt="">
              </div>
              <div style="width: 80%; text-align: left; margin-left: 2em;">
                <h3>Textual Analysis</h3>
                In our YouTube data, we will be working with a dense amount of textual data. 
                Therefore, we will first conduct an N-gram analysis, which will help us highlight meaningful words and word pairings to identify “trends” or word(s) that appear frequently across videos. 
                After doing so we will also run the Latent Dirichlet Allocation which is a statistical generative algorithm that will group our words and give prevalence and saliency to specific words based on their probability distribution.
              </div>
            </div>

            <div style="display: flex; margin: auto; width: 80%; margin-bottom: 2em; margin-top: 3em;">
              
              <div style="width: 80%; text-align: right; margin-right: 2em;">
                <h3>Time Series Analysis</h3>
                In the second part of our analysis, we will determine the density of specific “events” across our available time series. 
                For example, if the COVID-19 vaccine was developed in 2020, we are curious to see if keywords like “COVID-19”, “vaccine”, or any words associated were seen across more videos during that time period. 
                We try to pick events that are drastically different in category to get a good idea of the spread and diversity in the coverage of different historical events and their prevalence on YouTube.
              </div>
              <div style="width: 40%;">
                <img src="./img/timeseries.png" class="img-fluid" alt="">
              </div>
            </div>
          </p>
        </div>

        <div style="text-align: center; font-size: 100px; margin-top: -1em; margin-bottom: -0.2em;">---</div>

      </div>
    </section><!-- End Introduction Section -->

    <!-- ======= Facts Section ======= -->
    <section id="data" class="facts">
      <div class="container" data-aos="fade-up">

        <div class="section-title">
          <h2>Our data in numbers</h2>
          <p>Here are some of the numbers from the YouNiverse dataset. </p>
        </div>

        <div class="row">

          <div class="col-lg-4 col-md-6">
            <div class="count-box">
              <i class="bx bx-globe"></i>
              <span data-purecounter-start="0" data-purecounter-end="136000" data-purecounter-duration="1" class="purecounter"></span>
              <p>Number of YouTube Channels</p>
            </div>
          </div>

          <div class="col-lg-4 col-md-6 mt-5 mt-lg-0">
            <div class="count-box">
              <i class="fa-brands fa-youtube"></i>
              <span data-purecounter-start="0" data-purecounter-end="72900000" data-purecounter-duration="1" class="purecounter"></span>
              <p>Number of YouTube Videos</p>
            </div>
          </div>

          <div class="col-lg-4 col-md-6 mt-5 mt-md-0">
            <div class="count-box">
              <i class="fa-solid fa-memory"></i>
              <span data-purecounter-start="0" data-purecounter-end="111.8" data-purecounter-duration="1" class="purecounter"></span>
              <p>Memory Size of Compressed Dataset (GB)</p>
            </div>
          </div>
      </div>
    </section><!-- End Facts Section -->

    <!-- ======= Youniverse Section ======= -->
    <section id="data" class="about smallerWidthDisplay">
      <div class="container" data-aos="fade-up">
        <div class="section-title">
          <h2>What is YouNiverse?</h2>

          <div class="youtube usage">
            <div id="map1" class="map_containers">
              <!-- Title above map -->
              <object type="text/img" data="./img/logo.png" align="right" width=820, height=200></object>
            </div>
          </div>

        </div>
        <p style="text-align: left;">
          <br></br>
          <br></br>
          <br></br>
          <br></br>
          <br></br>
          In our project, we leveraged the YouNiverse dataset, which is a large collection of the channel and video metadata from English-language YouTube. 
          This dataset comprises metadata from over 136k channels and 72.9M videos published between May 2005 and October 2019, as well as channel-level time-series data with weekly subscriber and view counts. 
          Within this dataset, we also get information from the online service <a href="https://socialblade.com/" target="_blank">Social Blade</a>, which allows users to assess and enhance the representativeness of the sample of channels to rank channels accordingly. 
          YouNiverse is publicly available and more information can be found at this <a href="https://zenodo.org/record/4650046#.Y5jcgOzMIpw" target="_blank">link</a>.
        </p>
      </div>
    </section>

    <section id="data2" class="about smallerWidthDisplay">
      <div class="container" data-aos="fade-up">
        <div>
          <div class="section-title">
            <h2>Meaningful data from YouNiverse</h2>
          </div>
          
          <p style="text-align: left;">
            Amongst the Youniverse dataset, the most critical component of our analysis lies within the YouTube metadata. 
            In this metadata lies a lucrative amount of features, but the ones of interest for our analysis are the <code>categories, description, title </code>& <code>upload_date</code>. 
            We briefly describe each of their value in our analysis pipeline:

            
            <div style="margin-left: 4em;">
              * <code>categories</code> - In our analysis, the categories feature allows us to look at specific video categories to filter out our data.
              * <code>description</code> - the description feature gives us the meat of our text data which describes our video.
              * <code>title</code> - The title feature is also critical to our analysis as titles are what attract users to click on videos.
              * <code>upload_date</code> - In our time series analysis, we often filter our YouTube videos by year to get the videos associated with the proper time frame. 
            </div>
          </p>

          <p style="text-align: left;">
            Within our data analysis pipeline, we have several data-generating code blocks that take the <code>yt_metadata_en.jsonl.gz</code> JSON files and extract the information needed to run our analysis. 
            For more information on the preprocessing, please check out our  <a href=#links" target="_blank">links section</a> which contains <code>Jupyter notebooks</code> that are annotated for the readability, usability, and reproducibility of our results.
          </p>
          
          </p>
        </div>
    </section><!-- End Youniverse Section -->


    <!-- ======= N-grams Section ======= -->
    <section id="ngrams" class="about smallerWidthDisplay">
      <div class="container" data-aos="fade-up">

        <div class="section-title">
          <h2>N-grams</h2>
        </div>

        <div class="section-title" style="margin-top: 2em;">
          <p style="text-align: left;">
            Our primary task at hand is working with the textual data that is made available from the <code>title</code> and <code>description</code> of each YouTube video from our metadata. 
            These features give us an idea of what the YouTube videos are about and help us contextualize specific subjects relating to the video. 
            Therefore, one of the most common and simplest methods in natural language processing when working with a massive corpora (collection of written texts) are N-grams.
            <br></br>
          
            In <em>N</em>-gram analysis, we select a number <em>N</em> which will take <em>N</em> sequential words and form a new corpora of all the possible <em>N</em>-long word pairings.
            Take a look at the pseudocode below which covers a very basic example: 
            <br></br>
          </p>

          <div>
            <ul>
              <li> <code># if <em>N</em>=2 </code></li>
              <li><code>sentence = “I was happy”</code></li>
              <li><strong><code>corpus = [I_was, was_happy]</code></strong></li>
            </ul>
          </div>

          <p style="text-align: left;">
            You can then count the frequency that these word pairings have formed to assess trends across our text data. 
            So in summary at a high level, we are breaking down large portions of our corpora data into more meaningful segments (<em>N</em>-gram) that help to identify the root cause behind what we call “trends”. 
            <br></br>

            So in our project, we took a particular emphasis on working with the <code>Politics & News</code> YouTube category. 
            Because textual data is often quite noisy, and a lot of preprocessing is required, we thought it would make sense to focus on one particular YouTube category. 
            So in our analysis, we cleaned up our 2005-2019 <code>Politics & News</code> related videos by removing all stop words (English and custom stop words), URLs, emojis, and punctuation. 
            We then formed unigram, bigram, and trigram corpora. 
            Once these steps were completed, we took advantage of the <code>word cloud</code> python framework which allows us to plot the most popular <em>N</em>-grams determined by their size and their boldness. 
            Below, you will see <code>GIFs</code> of our <code>Politics & News</code> specific word clouds. 

            <div class="youtube usage">
              <div id="map1" class="map_containers" style="text-align: right">
                <!-- Title above map -->
                <object type="text/gif" data="./img/unigram.gif" loop=infinite align="right" width=850, height=500 ></object>
                <object type="text/gif" data="./img/bigram.gif" loop=infinite align="right" width=850, height=500 ></object>
                <object type="text/gif" data="./img/trigram.gif" loop=infinite align="right" width=850, height=500 ></object>
              </div>
            </div>

          </p>
        </div>
    </section>
          
    <section id="futureWork" class="about smallerWidthDisplay">
      <div class="container" data-aos="fade-up">
        <div class="section-title">
          <h2> A closer look at 2016</h2>
        </div>
        

        <div class="parent_simon">
          <div>   
          <img src="./img/2016_bigram.png">
          
          </div>
          
          <div>
          
          <img src="./img/2016_unigram.png">   
          </div>
          
          </div>

        <div class="section-title" style="margin-top: 2em;">
          <p style="text-align: left;">
            2016 was a big year in the <code>Politics & News</code> world because of the contentious presidential election between Donald Trump & Hillary Clinton. 
            Involving third parties like Russia and others, we expected that our <em>N</em>-grams would put a particular emphasis on these events. 
            Interestingly enough, we found that the unigram was not very informative of the political landscape from that year. 
            The trigrams were also uninformative, so we did not picture them in this analysis. 
            While we expected words like <code>“trump”</code> and <code>“clinton”</code> to be dominant within our unigrams, we see that words with little relevance like <code>“watch”</code>, <code>“latest”</code>, and <code>“world”</code> were considered the trendiest words. 
            This is just one of the many challenges of working with textual data but also highlights the inability of unigrams to capture trends.
            <br></br>

            While the unigram (& trigram) analysis was rather uninformative, we can see that the bigrams were able to capture more, simply by changing this hyperparameter <em>N</em> to 2. 
            Words like <code>“donald_trump”</code> and <code>“hilary_clinton”</code> did make a bigger appearance across all YouTube videos from this year, and we think it's a better demonstration of this political year. 
            In addition, something interesting we found was India’s YouTube presence that starts to become frequent, especially in recent years. 
            And this is correlating to our first plots describing YouTube’s usage by country. 
            Therefore, words like <code>“hindi”</code>, people like <code>“prasar_bharati”</code> and channels like <code>“india_24x7”</code> made a substantial appearance across <code>Politics & News</code> YouTube.

          </p>
    </section><!-- N grams Section -->



     <!-- ======= Quotes Example Section ======= -->
     <section id="testimonials" class="testimonials section-bg smallerWidthDisplay">
      <div class="container" data-aos="fade-up">

        <div class="subTitle subTitleOfSection">
          <h2>2016 YouTube Titles and Descriptions of videos containing the word <strong>India</strong></h2>
        </div>

        <div class="testimonials-slider swiper" data-aos="fade-up" data-aos-delay="100">
          <div class="swiper-wrapper">

            <div class="swiper-slide">
              <div class="testimonial-item">
                <p>
                  <i class="bx bxs-quote-alt-left quote-icon-left"></i>
                  "<strong>India’s Reliance Defense Signs $6B Arms Deal for S-400 Missiles With Russia</strong> - <strong>India</strong>’s Reliance Defense Deal for S-400 Missiles With Russia" (August 11, 2016)
                  <i class="bx bxs-quote-alt-right quote-icon-right"></i>
                </p>
              </div>
            </div><!-- End testimonial item -->

            <div class="swiper-slide">
              <div class="testimonial-item">
                <p>
                  <i class="bx bxs-quote-alt-left quote-icon-left"></i>
                  "<strong>Four killed, thousands evacuated in Indian cyclone</strong> - Cyclone Vardah hits the southeast coast of <strong>India</strong>, killing at least four people and forcing tens of thousands to evacuate." (December 13, 2016)
                  <i class="bx bxs-quote-alt-right quote-icon-right"></i>
                </p>
              </div>
            </div><!-- End testimonial item -->

            <div class="swiper-slide">
              <div class="testimonial-item">
                <p>
                  <i class="bx bxs-quote-alt-left quote-icon-left"></i>
                  "<strong>India coal mine collapsed: Death toll rises to seven</strong> - Seven people were reported dead and several others feared trapped under, after a coal mine in Jharkhand's Godda district, <strong>India</strong> collapsed on Thursday night." (December 30, 2016)
                  <i class="bx bxs-quote-alt-right quote-icon-right"></i>
                </p>
              </div>
            </div><!-- End testimonial item -->

          </div>
          <div class="swiper-pagination"></div>
        </div>

      </div>
    </section><!-- End Quotes Example Section -->

    <!-- ======= LDA Section ======= -->
    <section id="lda" class="about smallerWidthDisplay">
      <div class="container" data-aos="fade-up">

        <div class="section-title">
          <h2>Word Relevance through Latent Dirichlet Allocation (LDA)</h2>
        </div>

        <div class="section-title" style="margin-top: 2em;">
          <p style="text-align: left;">
            In the next major component of our analysis, we take on a more sophisticated method from natural language processing known as Latent Dirichlet Allocation (LDA). 
            LDA is a generative statistical model that explains a set of observations (in our case, a dictionary of words) and produces a distribution of groupings over the items being clustered. 
            So while it sounds very much like a clustering algorithm, we can think of the algorithm as assigning a probability that a word belongs to a topic vs. a word belonging to a cluster. 
            And while the use case for this method is typically for topic modeling, we are going to leverage this algorithm to group together salient (most noticeable or important) keywords and provide an adjustable relevance score to particular words within a topic. 
            Before we discuss our LDA model, we would first like to discuss the algorithm in greater detail in the context of our problem.
          </p>
        </div>
      </div>
    </section>


    <!-- ======= Impact regression Section ======= -->
    <section id="lda2" class="about smallerWidthDisplay">
      <div class="container" data-aos="fade-up">
        <div class="section-title">
          <h2>LDA Algorithm</h2>
        </div>
          <div class="row" style="margin-top: -1em;">
            <div class="col-lg-1"></div>
            <div class="col-lg-10">
              <img src="./img/lda.png" class="img-fluid" alt="">
            </div>
          </div>

            <strong>1.</strong> We first generate a dictionary of words from all the YouTube videos and a list of words with their frequency counts. 
            <br></br>
            <strong>2.</strong> We then go through each document (YouTube videos: title + description) and randomly assign each word in the document to one of the <strong>k topics</strong> (k is chosen beforehand). In our case, we chose k = 25, a number not too large but not too small to get a good spread of possible topics.
            <br></br>
            <strong>3.</strong> Next for each <strong>document <em>d</em></strong>, go through each <strong>word <em>w</em></strong> and compute:
            <br></br>
            <div style="margin-left: 4em;">
              <strong>a. p(topic t | document d): Calculate the proportion of words from the document that are assigned to one of our topics. </strong> By calculating this we capture how many words belong to the topic of a given document. For example, if a lot of words from a random YouTube document (video) belong to the <code>Politics & News</code> topic, it is more probable that these words belong to the <code>Politics & News</code> topic.
              <br></br>
              <strong>b. p(word w| topic t): Calculate the proportion of all the assignments of a given topic over all documents that come from a specific word. This tries to capture how many documents are in topic t because of the word w. </strong> And since LDA visualizes documents as a mixture of topics, we can imply that the topics are a mixture of words. So if the word <code>“Donkey Kong”</code> has a huge probability of being in the topic <code>Video Games</code>, all the documents having this word will strongly be associated with this topic as well. 
                Similarly, if the word <code>“Fauci”</code> is not probable to be in the topic <code>Music</code>, the probability assigned to <code>“Fauci”</code> will be low because the rest of the words in the document will belong to some other topic and hence the document will have a higher probability for those topics. 
                <br></br>
            </div>

          <div>
            <p style="text-align: left;">
              After computing the probabilities of each word, we arrive at our completed LDA model. 
              Computationally, we were able to achieve this by using the <code>gensim</code> framework, which prepared our documents and fed them into the Latent Dirichlet Allocation model to be trained. 
              Once training was completed, we took advantage of the <code>LDAvis</code> Python framework, which provides us with an interactive plot of our topics and relevant/salient words associated with each topic. 
              However, for the sake of time and our sanity, we would like to note that the LDA model was trained on a random sample of 1,000,000 YouTube videos. 
              Since the training was done on our local machines, our memory (RAM) plays a significant role in its time complexity and with just 8 GB of RAM, we did the best we could. 
              (<em>Note that although 1,000,000 videos feels like a small sample size, the dictionary built from a corpora of 1,000,000 videos can be possibly exponentially larger.</em>)
              Below, you can interact with our latent Dirichlet Allocation model.
            </p>
          </div>
      

      </div>
    </section><!-- LDA Section -->

      <div class="youtube usage">
        <div id="map1" class="map_containers">
          <!-- Title above map -->
          <object type="text/html" data="./img/lda.html" width=1200, height=1000 class="center_image_3"></object>
        </div>
      </div>

    

    <!-- ======= Timeseries Analysis Section ======= -->
    <section id="timeseries" class="about smallerWidthDisplay">
      <div class="container testimonials" data-aos="fade-up">

        <div class="section-title">
          <h2>Time Series Analysis</h2>
          <p style="text-align: left;">
            Sentiment analysis is a text analysis method that detects polarity (e.g. a positive or negative opinion) within the text. 
            We did a Sentiment analysis on Elon's quotes using VADER, a model used for text sentiment analysis that is sensitive 
            to polarity (positive/negative) and intensity (strength) of emotion. The analysis returns a sentiment score that is a 
            value between -1 and 1, where -1 is very negative, 0 is neutral and 1 is very positive.
            We used VADER to score Elon's quotes to whether his comments are positive or neutral or negative, 
            which then can be used to see the effect his comments had on the stock price and the popularity of the mentioned company. 
          </p>
        </div>

        <div style="text-align: center; width: 80%; margin: auto; margin-top: 1em; margin-bottom: 3em;">
          <i class="bx bxs-quote-alt-left quote-icon-left"></i>
          They have hired people we've fired. We always jokingly call Apple the Tesla graveyard. If you don't make it at Tesla, you go work for Apple.
          <i class="bx bxs-quote-alt-right quote-icon-right"></i>
        </div>
        
        <p style="margin-top: 1em;">
          This particular quote had a sentiment score of: <b>-0.7003</b>. 
          This is expected as he mentions Apple negatively in this particular quote. 
          So now let us take a closer look at how Apple's popularity changed after this quote. 
          The black vertical line displays the date of the Elon quote. From the figure below, 
          which uses Google trends, one can think that Elon's quote did not affect the popularity of 
          search about Apple. But we will make further analysis to prove this hypothesis. We have included 
          the graph for iPhone as a comparison to the Apple graph to make sure the Apple trend is related to 
          the technology company and not the fruit. As you can see, there is a minimal increase. 
          This might actually tell us that Elon does not have that big of an impact on 
          Apple and we believe that this can be generalized to larger companies as well.
        </p>

        <div class="row" style="margin-top: -1em;">
          <div class="col-lg-1"></div>
          <div class="col-lg-10">
            <img src="assets/img/ggletrend_quote1.svg" class="img-fluid" alt="">
          </div>
        </div>

        <p style="margin-top: 1em;">
          In this example, it did not seem like Elon affected the popularity of Apple when he mentioned them. 
          It increased a little bit, but not significantly. Now that we have looked at the popularity, 
          let's take a look at the stock price of Apple. We expect it to drop, as Elon talks badly about Apple. 
          The figure below shows the development of Apple stock. Once again the black vertical line displays the date 
          of the Elon quote. We can see that the stock price actually drops, but not significantly. So this is probably not 
          because of the Elon quote. We could also see that the stock price increased quite a lot the day before, and 
          therefore it might not be strange that it fell a little bit the day after. 
          <a href="https://www.tiderpenger.no/" target="_blank">
          Typically when a stock increases a lot one day, the next day it usually decreases a little bit as some people might sell 
          due to the increase in price and they want to secure their gain</a>. So the fact that Apple's price dropped a 
          bit is most likely not because of Elon. This might tell us that Elon does not have that big of an impact on the 
          stock price of large-cap companies.
        </p>

        <div class="row" style="margin-top: -2em;">
          <div class="col-lg-1"></div>
          <div class="col-lg-10">
            <img src="assets/img/yfinance_quote1.svg" class="img-fluid" alt="">
          </div>
        </div>

        <div style="margin-top: 4em">
          <h2>Regression based on sentiment analysis</h2>
          <p style="text-align: left;">
            Here we wanted to see if there is a significant effect of Musk's quotes on a company's stock price and popularity. 
            To achieve this, we plotted the percentage change in stock price or percentage change popularity against the sentiment 
            score. As a first analysis, for all cases, we observe that the data is dispersed and sparse. Doing a regression on this 
            type of data is difficult because we don't know which model to use. We decided to model the change of stock price 
            (or popularity) with a linear model with only one feature. The feature used is the score of the sentiment analysis 
            for each quote. One can also see that when fitting this model, the R-squared of the model is very low. 
            This implies that the regression model fits the change of stock (or popularity) very badly. 
            This means that we cannot say that Elon quotes have an effect. We can state that the Elon quotes cannot describe 
            the change in stock prices (or popularity). This is normal since we only used one feature. This might be not enough 
            as there are important variables to take into account when we are talking about stock prices (or popularity). 
          </p>
        </div>

        <div>
          <div class="row">
            <div class="col-lg-6">
              <img src="assets/img/sensitivity/Apple_stock.png" class="img-fluid" alt="">
            </div>
            <div class="col-lg-6">
              <img src="assets/img/sensitivity/Apple_pop.png" class="img-fluid" alt="">
            </div>
          </div>

          <div class="row">
            <div class="col-lg-6">
              <img src="assets/img/sensitivity/Ford_stock.png" class="img-fluid" alt="">
            </div>
            <div class="col-lg-6">
              <img src="assets/img/sensitivity/Ford_pop.png" class="img-fluid" alt="">
            </div>
          </div>

          <div class="row">
            <div class="col-lg-6">
              <img src="assets/img/sensitivity/Tesla_stock.png" class="img-fluid" alt="">
            </div>
            <div class="col-lg-6">
              <img src="assets/img/sensitivity/Tesla_pop.png" class="img-fluid" alt="">
            </div>
          </div>
                    
        </div>

      </div>
    </section><!-- End Timeseries Analysis Section -->

    <!-- ======= Discussion Section ======= -->
    <section id="discussion" class="about smallerWidthDisplay">
      <div class="container" data-aos="fade-up">

        <div class="section-title">
          <h2>Discussion</h2>
        </div>

        <div>
          <h2>Matching</h2>

          <p style="text-align: left;">
            One way to find out whether or not Elon has an impact on the stock price of different companies is to perform an 
            observational study. In an observational study the researcher, in our case us, observe the effect of a risk factor 
            simply by using raw data and not preform a randomized experiment.
            <br></br>

            The idea is to see if Elon's quotes has an effect on the stock price. So in order to find that out, we will observe 
            the change in stock price of the company that Elon has mentioned, and compare that change to the change in stock price 
            of other similar companies that was not mentioned by Elon. By doing this for multiple companies and at different dates 
            we can see if it is an actual Elon Effect or not. The number of different comparisons will increase the probability of 
            the result being true.
            <br></br>

            We used <a href="https://pypi.org/project/yfinance/" target="_blank">yfinance<a> a python library that allows you to easily 
            get information from Yahoo finance, for the financial information in the observational study. 
            In our case the quoted companies represent the “treated” population of the observational study. 
            To find a control population, we first select companies that we think are similar to the quoted company 
            which are not quoted on the same date (and quoted little in general). To perform the matching that will 
            optimize our model, we decide to introduce covariates for each day that can be observed such as the:
            <br></br>
          </p>

          <ul style="text-align: left;">
            <li>Close, stock price of the company at the end of the day.</li>
            <li>Volume, how many stock has been exchanged during the day.</li>
            <li>MarketCap, the value of the company at the end of the day.</li>
            <li>Popularity, represented by the number of actual search requests made to Google during this day on the company.</li>
            <li>Money Volume which represents the product of the close and the volume.</li>
          </ul>

          <p style="text-align: left;">
            These values are obtained through yahoo finance and google. We also add columns not depending on time: 
            <br></br>
          </p>

          <ul style="text-align: left;">
            <li>Elon: takes value 1 or 0 to indicate if Elon Musk talk about the company.</li>
            <li>Compare: for the controlled companies, it takes the name of the company we want to compare it with. And for the treated ones, take 'None' as value.</li>
          </ul>          

          <br></br>
          <p style="text-align: left;">
            <b>The following table represents the dataframe obtained and standardized:</b>
          </p>
          <br>
        
          <div>
            <table>
              <tr>
                <th>Name</th>
                <th>Date</th>
                <th>Close</th>
                <th>Volume</th>
                <th>MarketCap</th>
                <th>Popularity</th>
                <th>Elon</th>
                <th>Money Volume</th>
                <th>Propensity score</th>
              </tr>
              <tr id="base">
                <td>Apple</td>
                <td>2015-02-05</td>
                <td>-0.077488</td>
                <td>3.363161</td>
                <td>-0.020694</td>
                <td>0.065745</td>
                <td>1</td>
                <td>0.013481</td>
                <td>9.999954e-01</td>
              </tr>
              <tr>
                <td>Microsoft</td>
                <td>2015-02-05</td>
                <td>-0.072314</td>
                <td>0.083692</td>
                <td>-0.034855</td>
                <td>-0.147734</td>
                <td>0</td>
                <td>-0.059075</td>
                <td>5.085224e-01</td>
              </tr>
              <tr>
                <td>Dell</td>
                <td>2015-02-05</td>
                <td>-0.084971</td>
                <td>-0.719630</td>
                <td>-0.058121</td>
                <td>-0.527252</td>
                <td>0</td>
                <td>-0.086382</td>
                <td>9.863380e-02</td>
              </tr>
              <tr>
                <td>IBM</td>
                <td>2015-02-05</td>
                <td>-0.036986</td>
                <td>-0.646309</td>
                <td>-0.050319</td>
                <td>-0.788171</td>
                <td>0</td>
                <td>-0.078658</td>
                <td>1.205988e-07</td>
              </tr>
              <tr>
                <td>Samsung</td>
                <td>2015-02-05</td>
                <td>0.041447</td>
                <td>-0.538745</td>
                <td>0.074025</td>
                <td>1.488939</td>
                <td>0</td>
                <td>-0.041529</td>
                <td>3.341405e-08</td>
              </tr>
            </table>
          </div>

          <br></br>
          <p style="text-align: left;">
            Thanks to these covariates, the propensity score is computed for each company at each given day. 
            Now, we can compute the matching between the controlled and the treated companies. 
            To do so, a bipartite graph is constructed with edges between the controlled and treated companies 
            that we wish to compare, and weights equal to the similarity of these two companies. We solve the 
            maximum bipartite matching to gain pairs. 
            <br><br>
            Here you can see the representation of the graph:
          </p>

          <div class="col-lg-12" style="width: 60%; margin: auto; margin-bottom: 2em;">
            <img src="assets/img/graph.png" class="img-fluid" alt="">
          </div>
            
          <p style="text-align: left;">
            The matching is then obtained and we can now go to the next step which 
            consists of comparing the change of the stock of the treated vs control.
            <br><br>
          </p>
        </div>


        <div style="margin-top: 1em;">
          <h2>Tests on observational study</h2>
          <p style="text-align: left;">
            To do the observational study we had to be careful about a couple of more things. 
            For example the financial data was sometimes listed in other currencies than USD. 
            The most prominent is Samsung in Korean Won which is about 1000x of USD. We then had to 
            convert all monetary features of these types of companies to USD.
            <br><br>

            This was done manually for the few companies in question, and by taking an estimated average of the 
            exchange rate over the time period for our data.
            <br><br>

            We then redid the propensity score for this part and got the coefficients as follows:
            <br><br>

            <b>Intercept</b> <span id="tab">-1.0469</span><br>
            <b>Money volume</b> <span id="tab">8.3449</span><br>
            <b>MarketCap</b> <span id="tab">-2.1203</span><br>
            <b>Popularity</b> <span id="tab">0.2211</span><br>
            
            <br><br>
            Based on the outputted propensity scores we did the matching for each local group of stocks.

            <br><br>
            The next part was modeling the stock price change. We loaded in all the stock data for the companies 
            we have in our model and looked at the 5 consecutive days after a quote for the matched company and the 
            analysed company. Sometimes a quote will be during a weekend or the following span will include days 
            without stock history, then our model will automatically just take the days that have stock history. 
            We then use the stock prices to calculate the daily change (previous day price/next day price).
            
            <br><br>
            By comparing the distributions between a matched and control stock we can test if they have a similar distribution. 
            We used a Mann-Whitley u test for the distributions. The null-hypothesis is that the distributions are the same. 
            Here you can see the daily change for the control and 'treated' companies where the x axis is the dates in 
            chronological order, but not to scale as well as the results for the Mann Whitley U tests:
          </p>

          <div class="row" style="margin-top: 2em;">
            <h3>Apple</h3>
            <p><b>P-value (IBM)</b> <span id="tab">0.7818969961187895</span></p>
            <p><b>P-value (Samsung)</b> <span id="tab">0.07267412867689323</span></p>
            <p><b>P-value (Lenovo)</b> <span id="tab">0.20719648581932448</span></p>
            <p><b>P-value (Microsoft)</b> <span id="tab">0.9309874465595671</span></p>
            <div class="col-lg-12">
              <img src="assets/img/observational/observationalApple.svg" class="img-fluid" alt="">
            </div>
          </div>

          <div class="row" style="margin-top: 2em;">
            <h3>Ford</h3>
            <p><b>P-value (Renault)</b> <span id="tab">0.037136256414410235</span></p>
            <p><b>P-value (General Motors)</b> <span id="tab">0.5285921958576796</span></p>
            <br><br><br>
            <p>
              There is two empty figures because Toyota and Stellantis was not matched with ford. 
              This might have been due to a large market cap and low trading volume.
            </p>            
            <div class="col-lg-12">
              <img src="assets/img/observational/observationalFord.svg" class="img-fluid" alt="">
            </div>
          </div>

          <div class="row" style="margin-top: 2em;">
            <h3>PayPal</h3>
            <p><b>P-value (Western Union)</b> <span id="tab">0.26275304114766074</span></p>
            <p><b>P-value (Euronet)</b> <span id="tab">0.48073111045562256</span></p>
            <p><b>P-value (American Express)</b> <span id="tab">0.9009714934164412</span></p>
            <p><b>P-value (Visa)</b> <span id="tab">0.534965034965035</span></p>
            <div class="col-lg-12">
              <img src="assets/img/observational/observationalPayPal.svg" class="img-fluid" alt="">
            </div>
          </div>

          <div class="row" style="margin-top: 2em;">
            <h3>Tesla</h3>
            <p><b>P-value (Daimler)</b> <span id="tab">0.22820421433941152</span></p>
            <p><b>P-value (BMW)</b> <span id="tab">0.01735656673757902</span></p>
            <p><b>P-value (Volkswagen)</b> <span id="tab">0.46236044154701805</span></p>
            <p><b>P-value (General Motors)</b> <span id="tab">0.907735912135226</span></p>
            <div class="col-lg-12">
              <img src="assets/img/observational/observationalTesla.svg" class="img-fluid" alt="">
            </div>
          </div>

          <div class="row" style="margin-top: 2em;">
            <h3>Twitter</h3>
            <p><b>P-value (Google)</b> <span id="tab">0.48484848484848486</span></p>
            <p><b>P-value (Facebook)</b> <span id="tab">0.5468909935184665</span></p>
            <p><b>P-value (Snapchat)</b> <span id="tab">0.19477620553450214</span></p>
            <p><b>P-value (Pinterest)</b> <span id="tab">0.1681531290926782</span></p>
            <div class="col-lg-12">
              <img src="assets/img/observational/observationalTwitter.svg" class="img-fluid" alt="">
            </div>
          </div>

          <p style="text-align: left;">
            <br><br>  
            There seem to be two tests with a significant result. The P-value in the Ford test with Renault is under 0.05 
            which means that we can reject that they come from the same distribution. The same applies to Tesla and BMW. 
            However, it's hard to conclude that Elon's quotes have a significant impact on the 'treated' companies. 
            That is because not all the hypotheses were rejected, and it can also be the case that different companies' 
            stocks move in different ways.
          </p>

        </div>

      </div>
    </section><!-- End Discussion  Section -->

    <!-- =======Conclusion Section ======= -->
    <section id="conclusion" class="about smallerWidthDisplay">
      <div class="container" data-aos="fade-up">

        <div class="section-title">
          <h2>Conclusion</h2>
          <p style="text-align: left;">
            In light of the above, we did not see that Elon had a significant impact on the stock prices. In fact, we conducted two analyses to check if Elon's quotes have an impact on the stock prices by constructing an observational study  and to represent the impact of the quotes on the change of the stock price, by performing a linear regression. 
            <br><br>
            As stated in our research questions we wanted to test if his impact (if he had any) was different for big and small companies. However, the quotes about smaller companies are not enough to make an analysis of his impact on them.  One can think before this study that his impact would be significant on larger companies, except maybe Tesla, where he has an important role and therefore probably a direct impact. Interestingly enough, Tesla is also the company that had the most significant result in the Mann-Whitley U test. One can also take into consideration that Elon Musk usually uses Twitter when he utters an opinion about different companies. Unfortunately, Quote Bank does not take into account the tweets, as said at the beginning. 
            <br><br>
            However, the following study proves that when taking the covariates that we defined, Elon quotes don't have an impact on the stock price regardless of the sentiment score of the quote.
          </p>
        </div>

      </div>
    </section><!-- End Conclusion Section -->

    <!-- ======= Future Work Section ======= -->
    <section id="futureWork" class="about smallerWidthDisplay">
      <div class="container" data-aos="fade-up">

        <div class="section-title">
          <h2>Future work</h2>
          <p style="text-align: left;">
            While our initial analysis helped provide insight into our original question, we also see room for expansion to explore even more alleyways to provide answers to our question.
            For example, we had some thoughts on addressing whether specific New's broadcasting channels heightened particular words when it came to controversial topics to gain higher viewership. 
            In a similar realm, we also thought of conducting an analysis on highlighting videos with a high dislike to like ratio to see whether particular "negative" or "controversial" words appeared across videos.
            However we decided to keep our analysis very much in one pathway with very clear intentions. 
          </p>
          <br>
          <p style="text-align: left;">
            In terms of improving some of our methodologies from our current analysis, we thought we could do a better job at cleaning up our N-gram analysis. 
            However due to scalability and the noisy tendencies of textual data with numerous "irrelevant" words, we decided to use more robust methods like Latent Dirachlett Allocation to overcome our mediocre results from the N-gram analysis.
            Also within our time series data, we could continue adding more trendy/interesting topics that could reflect our question of whether Youtube is a reflection of our society?
            However what often gets overlooked is the the filtering of this massive dataset which often is computationally expensive especially when having to load the memory heavy data into the Pandas framework, which requires us to load everything onto memory.
          </p>

          <br>
          <p style="text-align: left;">
            Ultimately as students, we were constrained by other obligations and time allotted to us, but we feel great about what we were able to accomplish from this lucrative dataset. 
            Therefore in totality we are greatful to have conducted enough data anlysis to allow us to address our question based on some of the things covered in the ADA course. 
            So we thank Professor Bob West and the ADA team for all the help throughout this winter semester!
          </p>
          <br>
        </div>

      </div>
    </section><!-- End Future work Section -->

    <!-- Team Section -->
    <section id="links" class="pt-20 pb-48">
      <!-- Section Container -->
      <div class="container mx-auto px-4" data-aos="fade-in">
        <!-- Text Box -->
        <div class="flex flex-wrap justify-center text-center mb-24">
          <div class="w-full lg:w-6/12 px-4">
            <h2 class="text-4xl font-semibold">Meet the Team</h2>
            <p class="text-lg leading-relaxed m-4 text-gray-600">
              Just 4 Life Science Students working on a Data Science Project...
            </p>
          </div>
        </div>
        <!-- Flex Row Container -->
        <div class="flex flex-wrap">
          <!-- Item -->
          <div class="w-full md:w-6/12 lg:w-3/12 lg:mb-0 mb-12 px-4" data-aos="fade-in">
            <div class="px-6">
              <!-- Image -->
              <img alt="..." src="./img/profile/simon.JPG"
                class="shadow-lg rounded-full max-w-full mx-auto" style="max-width: 120px;" />
              <div class="pt-6 text-center">
                <h5 class="text-xl font-bold">Simon Lee</h5>
                <p class="mt-1 text-sm text-gray-500 uppercase font-semibold">
                  Data Scientist
                </p>
                <!-- Social Media Buttons -->
                <div class="mt-6">
                  <button class="bg-gray-800 text-white w-8 h-8 rounded-full outline-none focus:outline-none mr-1 mb-1"
                    type="button" onclick="window.location.href='https://github.com/Simonlee711';">
                    <!-- Icon -->
                    <i class="fab fa-github-square"></i>
                  </button>
                  <button class="bg-pink-500 text-white w-8 h-8 rounded-full outline-none focus:outline-none mr-1 mb-1"
                    type="button" onclick="window.location.href='https://simonlee711.github.io/';">
                    <!-- Icon -->
                    <i class="bx bx-globe"></i>
                  </button>
                </div>
              </div>
            </div>
          </div>
          <!-- Item -->
          <div class="w-full md:w-6/12 lg:w-3/12 lg:mb-0 mb-12 px-4" data-aos="fade-in" data-aos-delay="400">
            <div class="px-6">
              <!-- Image -->
              <img alt="..." src="./img/profile/Arben.jpg"
                class="shadow-lg rounded-full max-w-full mx-auto" style="max-width: 120px;" />
              <div class="pt-6 text-center">
                <h5 class="text-xl font-bold">Arben Miftari</h5>
                <p class="mt-1 text-sm text-gray-500 uppercase font-semibold">
                  Data Scientist
                </p>
                <!-- Social Media Buttons -->
                <div class="mt-6">
                  <button class="bg-blue-600 text-white w-8 h-8 rounded-full outline-none focus:outline-none mr-1 mb-1"
                    type="button" onclick="window.location.href='https://github.com/arbnm';">
                    <!-- Icon -->
                    <i class="fab fa-github-square"></i>
                  </button>
                </div>
              </div>
            </div>
          </div>
          <!-- Item -->
          <div class="w-full md:w-6/12 lg:w-3/12 lg:mb-0 mb-12 px-4" data-aos="fade-in" data-aos-delay="600">
            <div class="px-6">
              <!-- Image -->
              <img alt="..." src="./img/profile/siyoung.png"
                class="shadow-lg rounded-full max-w-full mx-auto" style="max-width: 120px;" />
              <div class="pt-6 text-center">
                <h5 class="text-xl font-bold">Siyoung Lee</h5>
                <p class="mt-1 text-sm text-gray-500 uppercase font-semibold">
                  Data Scientist
                </p>
                <!-- Social Media Buttons -->
                <div class="mt-6">
                  <button class="bg-blue-400 text-white w-8 h-8 rounded-full outline-none focus:outline-none mr-1 mb-1"
                    type="button" onclick="window.location.href='https://github.com/siyounglee00';">
                    <!-- Icon -->
                    <i class="fab fa-github-square"></i>
                  </button>
                </div>
              </div>
            </div>
          </div>
          <!-- Item -->
          <div class="w-full md:w-6/12 lg:w-3/12 lg:mb-0 mb-12 px-4" data-aos="fade-in" data-aos-delay="800">
            <div class="px-6">
              <!-- Image -->
              <img alt="..." src="./img/profile/lina.JPG"
                class="shadow-lg rounded-full max-w-full mx-auto" style="max-width: 120px;" />
              <div class="pt-6 text-center">
                <h5 class="text-xl font-bold">Lina Bacha</h5>
                <p class="mt-1 text-sm text-gray-500 uppercase font-semibold">
                  Data Scientist
                </p>
                <!-- Social Media Buttons -->
                <div class="mt-6">
                  <button class="bg-red-600 text-white w-8 h-8 rounded-full outline-none focus:outline-none mr-1 mb-1"
                    type="button" onclick="window.location.href='https://github.com/LinaBacha';">
                    <!-- Icon -->
                    <i class="fab fa-github-square"></i>
                  </button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End of Team Section -->

    <!-- ======= Usefull Links Section ======= -->
    <section id="links2" class="services smallerWidthDisplay">
      <div class="container" data-aos="fade-up">

        <div class="section-title">
          <h2>Usefull links</h2>
        </div>

        <div class="row">

          <div class="col-lg-6 col-md-6 d-flex align-items-stretch mt-4 mt-md-0" data-aos="zoom-in" data-aos-delay="200">
            <div class="icon-box iconbox-orange" style="width: 100%">
              <div class="icon">
                <svg width="100" height="100" viewBox="0 0 600 600" xmlns="http://www.w3.org/2000/svg">
                  <path stroke="none" stroke-width="0" fill="#f5f5f5" d="M300,582.0697525312426C382.5290701553225,586.8405444964366,449.9789794690241,525.3245884688669,502.5850820975895,461.55621195738473C556.606425686781,396.0723002908107,615.8543463187945,314.28637112970534,586.6730223649479,234.56875336149918C558.9533121215079,158.8439757836574,454.9685369536778,164.00468322053177,381.49747125262974,130.76875717737553C312.15926192815925,99.40240125094834,248.97055460311594,18.661163978235184,179.8680185752513,50.54337015887873C110.5421016452524,82.52863877960104,119.82277516462835,180.83849132639028,109.12597500060166,256.43424936330496C100.08760227029461,320.3096726198365,92.17705696193138,384.0621239912766,124.79988738764834,439.7174275375508C164.83382741302287,508.01625554203684,220.96474134820875,577.5009287672846,300,582.0697525312426"></path>
                </svg>
                <i class="fab fa-github-square"></i>
              </div>
              <h4><a href="https://github.com/epfl-ada/ada-2022-project-chromegoldfish" target="_blank">Our Project Repository</a></h4>
              <p>This link takes you to our Repository of Data, and Jupyter Notebooks used to generate our data.</p>
            </div>
          </div>

          <div class="col-lg-6 col-md-6 d-flex align-items-stretch mt-4 mt-lg-0" data-aos="zoom-in" data-aos-delay="300">
            <div class="icon-box iconbox-pink" style="width: 100%">
              <div class="icon">
                <svg width="100" height="100" viewBox="0 0 600 600" xmlns="http://www.w3.org/2000/svg">
                  <path stroke="none" stroke-width="0" fill="#f5f5f5" d="M300,541.5067337569781C382.14930387511276,545.0595476570109,479.8736841581634,548.3450877840088,526.4010558755058,480.5488172755941C571.5218469581645,414.80211281144784,517.5187510058486,332.0715597781072,496.52539010469104,255.14436215662573C477.37192572678356,184.95920475031193,473.57363656557914,105.61284051026155,413.0603344069578,65.22779650032875C343.27470386102294,18.654635553484475,251.2091493199835,5.337323636656869,175.0934190732945,40.62881213300186C97.87086631185822,76.43348514350839,51.98124368387456,156.15599469081315,36.44837278890362,239.84606092416172C21.716077023791087,319.22268207091537,43.775223500013084,401.1760424656574,96.891909868211,461.97329694683043C147.22146801428983,519.5804099606455,223.5754009179313,538.201503339737,300,541.5067337569781"></path>
                </svg>
                <i class="bx bx-globe"></i>
              </div>
              <h4><a href="https://dlab.epfl.ch/teaching/fall2021/cs401/" target="_blank">Course webpage</a></h4>
              <p>This link takes you directly to the course webpage</p>
            </div>
          </div>

        </div>

      </div>
    </section><!-- End Usefull Links -->

  </main><!-- End #main -->

  <!-- ======= Footer ======= -->
  <footer id="footer">
    <div class="container">
      <h3>Is YouTube a mirror of Society?</h3>
      <p>Simon Lee, Arben Miftari, Siyoung Lee & Lina Bacha</p>
      <p style="font-style: normal; font-size: 25px; margin-top: -1em; color:#FFCE44"><b>chromeGoldFish</b></p>
      <img src="assets/img/meme.png" class="img-fluid" alt="" style="width: 30%; margin-top: -2em;">
      <div class="credits" style="margin: 1em;">
        <!-- All the links in the footer should remain intact. -->
        <!-- You can delete the links only if you purchased the pro version. -->
        <!-- Licensing information: [license-url] -->
        <!-- Purchase the pro version with working PHP/AJAX contact form: https://bootstrapmade.com/free-html-bootstrap-template-my-resume/ -->
        Designed by <a href="https://bootstrapmade.com/">BootstrapMade</a>
      </div>
    </div>
  </footer><!-- End Footer -->

  <div id="preloader"></div>
  <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/purecounter/purecounter.js"></script>
  <script src="assets/vendor/aos/aos.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
  <script src="assets/vendor/typed.js/typed.min.js"></script>
  <script src="assets/vendor/waypoints/noframework.waypoints.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>
